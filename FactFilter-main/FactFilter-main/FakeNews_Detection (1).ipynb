{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307cc84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa5e487",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fake=pd.read_csv(\"fake_Data.csv\")\n",
    "fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeef0939",
   "metadata": {},
   "outputs": [],
   "source": [
    "true=pd.read_csv(\"True_Data.csv\")\n",
    "true.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3704d85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e599c08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acc08b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d83c2190",
   "metadata": {},
   "source": [
    "## Data Cleaning And Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7354d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add flag to track fake and real\n",
    "fake['target'] = 'fake'\n",
    "true['target'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ba43d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate dataframes\n",
    "data = pd.concat([fake, true]).reset_index(drop = True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2080bf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "from sklearn.utils import shuffle\n",
    "data = shuffle(data)\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c66f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e06f5a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14344/1894126977.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Removing the date (we won't use it for the analysis)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"date\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# Removing the date (we won't use it for the analysis)\n",
    "data.drop([\"date\"],axis=1,inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5392c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the title (we will only use the text)\n",
    "data.drop([\"title\"],axis=1,inplace=True)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451fcbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to lowercase\n",
    "\n",
    "data['text'] = data['text'].apply(lambda x: x.lower())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062c4921",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1df814",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58a55c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "data['text'] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cb17c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7dd5c1",
   "metadata": {},
   "source": [
    "## Basic data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5b7cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many articles per subject?\n",
    "print(data.groupby(['subject'])['text'].count())\n",
    "data.groupby(['subject'])['text'].count().plot(kind=\"bar\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0ac1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many fake and real articles?\n",
    "print(data.groupby(['target'])['text'].count())\n",
    "data.groupby(['target'])['text'].count().plot(kind=\"bar\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4c6c7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843d0aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most frequent words counter (Code adapted from https://www.kaggle.com/rodolfoluna/fake-news-detector)   \n",
    "from nltk import tokenize\n",
    "\n",
    "token_space = tokenize.WhitespaceTokenizer()\n",
    "\n",
    "def counter(text, column_text, quantity):\n",
    "    all_words = ' '.join([text for text in text[column_text]])\n",
    "    token_phrase = token_space.tokenize(all_words)\n",
    "    frequency = nltk.FreqDist(token_phrase)\n",
    "    df_frequency = pd.DataFrame({\"Word\": list(frequency.keys()),\n",
    "                                   \"Frequency\": list(frequency.values())})\n",
    "    df_frequency = df_frequency.nlargest(columns = \"Frequency\", n = quantity)\n",
    "    plt.figure(figsize=(12,8))\n",
    "    ax = sns.barplot(data = df_frequency, x = \"Word\", y = \"Frequency\", color = 'blue')\n",
    "    ax.set(ylabel = \"Count\")\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f8ce1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most frequent words in fake news\n",
    "counter(data[data[\"target\"] == \"fake\"], \"text\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5438dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most frequent words in real news\n",
    "counter(data[data[\"target\"] == \"true\"], \"text\", 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a1d161",
   "metadata": {},
   "source": [
    "## Modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd963373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the confusion matrix \n",
    "from sklearn import metrics\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1373982",
   "metadata": {},
   "source": [
    "## Preparing the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9117be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train,X_test,y_train,y_test = train_test_split(data['text'], data.target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a4635e",
   "metadata": {},
   "source": [
    "## Logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a551f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing and applying TF-IDF\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('model', LogisticRegression())])\n",
    "\n",
    "# Fitting the model\n",
    "model = pipe.fit(X_train, y_train)\n",
    "\n",
    "# Accuracy\n",
    "prediction = model.predict(X_test)\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a21d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y_test, prediction)\n",
    "plot_confusion_matrix(cm, classes=['Fake', 'Real'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191f496c",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f25227",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Vectorizing and applying TF-IDF\n",
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('model', DecisionTreeClassifier(criterion= 'entropy',\n",
    "                                           max_depth = 20, \n",
    "                                           splitter='best', \n",
    "                                           random_state=42))])\n",
    "# Fitting the model\n",
    "model = pipe.fit(X_train, y_train)\n",
    "\n",
    "# Accuracy\n",
    "prediction = model.predict(X_test)\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949285f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y_test, prediction)\n",
    "plot_confusion_matrix(cm, classes=['Fake', 'Real'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a54ee1e",
   "metadata": {},
   "source": [
    "## Random Forest Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aa49e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipe = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('model', RandomForestClassifier(n_estimators=50, criterion=\"entropy\"))])\n",
    "\n",
    "model = pipe.fit(X_train, y_train)\n",
    "prediction = model.predict(X_test)\n",
    "print(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80d78e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y_test, prediction)\n",
    "plot_confusion_matrix(cm, classes=['Fake', 'Real'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35606fb",
   "metadata": {},
   "source": [
    "## TfidfVectorizer\n",
    "\n",
    "Convert a collection of raw documents to a matrix of TF-IDF features\n",
    "\n",
    "TF-IDF where TF means term frequency, and IDF means Inverse Document frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aac8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "text = ['Hello Hardik Sharma this side , I love machine learning','Welcome to the Machine learning hub' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d7d428",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355934fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect.fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b67018c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TF will count the frequency of word in each document. and IDF \n",
    "print(vect.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fa245f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vect.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa70d4c0",
   "metadata": {},
   "source": [
    " A word  which is present in all the data, it will have low IDF value. With this unique words will be highlighted using the Max IDF values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c950ca4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = text[0]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c60efb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = vect.transform([example])\n",
    "print(example.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0d7561",
   "metadata": {},
   "source": [
    "Here, 0 is present in the which indexed word, which is not available in given sentence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3f7bbe",
   "metadata": {},
   "source": [
    "PassiveAggressiveClassifier\n",
    "Passive: if correct classification, keep the model; Aggressive: if incorrect classification, update to adjust to this misclassified example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9691ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfvect = TfidfVectorizer(stop_words='english',max_df=0.7)\n",
    "tfid_x_train = tfvect.fit_transform(X_train)\n",
    "tfid_x_test = tfvect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb504d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = PassiveAggressiveClassifier(max_iter=50)\n",
    "classifier.fit(tfid_x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683a6c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(tfid_x_test)\n",
    "score = accuracy_score(y_test,y_pred)\n",
    "print(f'Accuracy: {round(score*100,2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c66bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_news_det(news):\n",
    "    input_data = [news]\n",
    "    vectorized_input_data = tfvect.transform(input_data)\n",
    "    prediction = classifier.predict(vectorized_input_data)\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dcbd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_det('U.S. Secretary of State John F. Kerry said Monday that he will stop in Paris later this week, amid criticism')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b452f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_det('On Friday, it was revealed that former Milwauk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed75e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(classifier,open('model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8076a50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "loaded_model = pickle.load(open('model.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd3e78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_news_det1(news):\n",
    "    input_data = [news]\n",
    "    vectorized_input_data = tfvect.transform(input_data)\n",
    "    prediction = loaded_model.predict(vectorized_input_data)\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135d2487",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_det1(''''Go to Article \n",
    "President Barack Obama has been campaigning hard for the woman who is supposedly going to extend his legacy four more years.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbea1477",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_det('''U.S. Secretary of State John F. Kerry said Monday that he will stop in Paris later this week, amid criticis''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbb2495",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a03e69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c155448",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
